{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"/Users/wvsharber/FlatironLessons/mod3project_Tanzania_water_well/\")\n",
    "from src.useful_func import do_everything, create_voting_classifier, create_graph, clean_data, drop_useless_cols, fix_dates, clean_data, bin_me, onehotmess\n",
    "os.chdir(\"/Users/wvsharber/FlatironLessons/mod3project_Tanzania_water_well/competition/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_cols(df, drop_values = []):\n",
    "    \"\"\"\n",
    "    Drops columns from df that are specificied in drop_values. Won't drop values from continuous data columns, but will raise an error if you try. Returns DataFrame with columns dropped.\n",
    "    \"\"\"\n",
    "    \n",
    "    continuous_columns = ['amount_tsh', 'date_recorded', 'gps_height', 'population', 'construction_year']\n",
    "    for cont in continuous_columns:\n",
    "        if cont in drop_values:\n",
    "            print(f'you cannot drop column: {cont}')\n",
    "            return\n",
    "        \n",
    "    try:\n",
    "        df_dropped = df.drop(drop_values, axis = 1)\n",
    "        return df_dropped\n",
    "    except:\n",
    "        return df\n",
    "    \n",
    "def load_data(string1, string2):\n",
    "    \"\"\"\n",
    "        Pass in two strings containg .csv file paths. This function will load the two dataframes and merge them along the column 'id'. Returns merged DataFrame.\n",
    "    \"\"\"\n",
    "    df_1 = pd.read_csv(string1)\n",
    "    df_2 = pd.read_csv(string2)\n",
    "    #merging dataframes\n",
    "    df = pd.merge(df_1, df_2, on = 'id', how = 'inner')\n",
    "    return df\n",
    "\n",
    "    \n",
    "def fix_dates(df):\n",
    "    \"\"\" \n",
    "    Takes the date of 01/01/2020 and subtracts it from the 'date_recorded' column. This information will be stored in column called 'days_since_recording' and drops the 'date_recorded' column from the DataFrame. Returns DataFrame.\n",
    "    \"\"\"\n",
    "    basedate = datetime(2020, 1, 1)\n",
    "    df['days_since_recording'] = df.loc[:,'date_recorded'].map(lambda x: (basedate - datetime.strptime(x, \"%Y-%m-%d\")).days)\n",
    "    df.drop(['date_recorded'], axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "def clean_data(df, threshold = 100):\n",
    "    \"\"\"\n",
    "    Replaces all NaN values in DataFrame with 'Not Known'. For categorical columns, replaces all values with a count less than 100 (threshold value) with 'other'. Returns edited DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # replaces NaN with a string 'not known'\n",
    "    df = df.fillna('Not Known')\n",
    "    \n",
    "    uvdict = {}\n",
    "\n",
    "    for column in df.select_dtypes(exclude=['int','float']):\n",
    "        values_list = df[column].unique()\n",
    "        uvdict[column] = len(values_list)\n",
    "\n",
    "    target_list = list(filter(lambda x: uvdict[x] > threshold, uvdict.keys()))\n",
    "                       \n",
    "                       \n",
    "    for col in target_list:\n",
    "        valued_dict = dict(df[col].value_counts())\n",
    "        safe_values = list(key for key, value in valued_dict.items() if value >= 50)\n",
    "    #     replace_values = list(filter(lambda x: x not in safe_values, all_values))\n",
    "        df.loc[:, col] = df.loc[:, col].map(lambda y: 'other' if y not in safe_values else y)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def bin_me(df):\n",
    "    \"\"\"\n",
    "    Creates bins for construction_year based on 5 year increments. In addition, values stored as year 0 will be transformed to 'not_available'. Returns edited DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        basedate = datetime(2020, 1, 1)\n",
    "        a = list(range(1955,2016,5))\n",
    "        cut_bins = [-1]\n",
    "        cut_bins.extend(a)\n",
    "        cut_labels = ['not available', '56-60','61-65','66-70','71-75','76-80','81-85','86-90','91-95','96-00','01-05','06-10','11-15']\n",
    "        df.loc[:, 'construction_year_bin'] = pd.cut(df['construction_year'], bins = cut_bins, labels = cut_labels)\n",
    "        df.drop(['construction_year'], axis = 1, inplace = True)\n",
    "        return df\n",
    "    except:\n",
    "        if 'construction_year_bin' in df.columns:\n",
    "            print('action already performed')\n",
    "        else:\n",
    "            print('you messed up')\n",
    "\n",
    "def onehotmess(df):\n",
    "    \"\"\"\n",
    "    Uses pd.getdummies() to one hot encode categorical variables in DataFrame. Returns edited DataFrame and target DataFrame.\n",
    "    \"\"\"\n",
    "    df_objects = df.select_dtypes(exclude=['int','float']).drop(['status_group'], axis = 1)\n",
    "    df_nums = df.select_dtypes(include=['int','float'])\n",
    "\n",
    "    df_onehot = pd.get_dummies(df_objects)\n",
    "\n",
    "    df_final = pd.concat([df_nums, df_onehot], axis = 1)\n",
    "    \n",
    "    return df_final, df.status_group\n",
    "\n",
    "def normalize_func(df_values, df_target):\n",
    "    \"\"\"\n",
    "    Takes DataFrame of training data and target values, performs a train-test split, and then scales the data using MinMaxScaler. Returns train and test sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, test_size = .05, random_state = 42)\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_ = scaler.fit_transform(X_train)\n",
    "    X_test_ = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_, X_test_, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoder(df):\n",
    "    df_objects = df.select_dtypes(exclude=['int','float']).drop(['status_group'], axis = 1)\n",
    "    df_nums = df.select_dtypes(include=['int','float'])\n",
    "    \n",
    "    ohe = OneHotEncoder()\n",
    "    onehot_array = ohe.fit_transform(df_objects)\n",
    "    df_onehot = pd.DataFrame(onehot_array, columns = ohe.columns)\n",
    "    \n",
    "    df_final = pd.concat([df_nums, df_onehot], axis = 1)\n",
    "    \n",
    "    return df_final, df.status_group, ohe\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_everything(string1, string2, drop_values, thresh = 200):\n",
    "    \"\"\"\n",
    "    This function wraps previously defined data cleaning and preprocessing functions and returns processed train and test data sets.\n",
    "    \"\"\"\n",
    "    loaded_data = load_data(string1, string2)\n",
    "    df_dropped = drop_useless_cols(loaded_data, drop_values)\n",
    "    fixed_date = fix_dates(df_dropped)\n",
    "    cleaner_df = clean_data(fixed_date, thresh)\n",
    "    df_binned = bin_me(cleaner_df)\n",
    "    ohe_df, target_df, ohe = onehotencoder(df_binned)\n",
    "    X_train, X_test, y_train, y_test, scaler = normalize_func(ohe_df, target_df)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, ohe_df, target_df, ohe, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_1 = '../data/training_data_values.csv' #Your filepath to the data here\n",
    "string_2 = '../data/training_data_labels.csv' #Your filepath to the data here\n",
    "drop_columns = ['id', 'funder', 'num_private', 'longitude', 'latitude', 'wpt_name', 'subvillage', 'region_code', 'lga',\n",
    "                'ward','recorded_by', 'scheme_name', 'extraction_type_group', 'payment', 'quality_group', \n",
    "                'quantity_group', 'source_type', 'source_class', 'waterpoint_type_group']\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test, df_values, df_target, ohe, scaler = do_everything(string_1, string_2, drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = load_data(string_1, string_2)\n",
    "df_dropped = drop_useless_cols(loaded_data, drop_columns)\n",
    "fixed_date = fix_dates(df_dropped)\n",
    "cleaner_df = clean_data(fixed_date, 200)\n",
    "df_binned = bin_me(cleaner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coerced = df_binned.select_dtypes(exclude=['int','float']).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_binned['status_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coerced.drop(['status_group'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(df_coerced)\n",
    "onehot_array = ohe.transform(df_coerced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, scaler = normalize_func(onehot_array, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF1 = RandomForestClassifier(n_estimators = 200)\n",
    "LR1 = LogisticRegression(max_iter = 1000)\n",
    "GBR1 = GradientBoostingClassifier(n_estimators = 300)\n",
    "KNN1 = KNeighborsClassifier()\n",
    "\n",
    "#Hard-coded estimates of accuracy from previously fit models\n",
    "lr_weight, rf_weight, gbr_weight, knn_weight = .77, .80, .78, .78\n",
    "\n",
    "eclf_soft = VotingClassifier(estimators = [('lr', LR1),\n",
    "                                           ('rf', RF1),\n",
    "                                           ('gbr', GBR1),\n",
    "                                           ('knn', KNN1)],\n",
    "                             weights = [lr_weight, rf_weight, gbr_weight, knn_weight],\n",
    "                             voting = 'soft')\n",
    "eclf_soft.fit(X_train, y_train)\n",
    "score = eclf_soft.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.806060606060606"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/test_data_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 200\n",
    "\n",
    "test_df_dropped = drop_useless_cols(test_df, drop_columns)\n",
    "test_fixed_date = fix_dates(test_df_dropped)\n",
    "test_cleaner_df = clean_data(test_fixed_date, thresh)\n",
    "test_df_binned = bin_me(test_cleaner_df)\n",
    "test_df_coerced = test_df_binned.select_dtypes(exclude=['int','float']).astype('str')\n",
    "test_onehot_array = ohe.transform(test_df_coerced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = scaler.transform(test_onehot_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = eclf_soft.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.concat([test_df['id'], pd.DataFrame(test_preds, columns = ['status_group'])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.to_csv(\"test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "water-env",
   "language": "python",
   "name": "water-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
