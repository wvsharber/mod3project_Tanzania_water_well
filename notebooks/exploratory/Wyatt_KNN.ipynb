{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(string1, string2):\n",
    "    \"\"\"\n",
    "        pass in two strings containg csv info, this function will load the two dataframes and merge them along the column 'id'\n",
    "    \"\"\"\n",
    "    df_1 = pd.read_csv(string1)\n",
    "    df_2 = pd.read_csv(string2)\n",
    "    #merging dataframes\n",
    "    df = pd.merge(df_1, df_2, on = 'id', how = 'inner')\n",
    "    return df\n",
    "\n",
    "def drop_useless_cols(df, drop_values = []):\n",
    "    continuous_columns = ['amount_tsh', 'date_recorded', 'gps_height', 'population', 'construction_year']\n",
    "    for cont in continuous_columns:\n",
    "        if cont in drop_values:\n",
    "            print(f'you cannot drop column: {cont}')\n",
    "            return\n",
    "        \n",
    "    try:\n",
    "        df_dropped = df.drop(drop_values, axis = 1)\n",
    "        return df_dropped\n",
    "    except:\n",
    "        return df\n",
    "    \n",
    "def fix_dates(df):\n",
    "    \"\"\" will take the date of 01/01/2020 and subtract it from the 'date_recorded' column.\n",
    "        This information will be stored in column called 'days_since_recording'\n",
    "        This will also drop the 'date_recorded' column\n",
    "    \"\"\"\n",
    "    basedate = datetime(2020, 1, 1)\n",
    "    df['days_since_recording'] = df.loc[:,'date_recorded'].map(lambda x: (basedate - datetime.strptime(x, \"%Y-%m-%d\")).days)\n",
    "    df.drop(['date_recorded'], axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "def clean_data(df, threshold = 100):\n",
    "    # replaces NaN with a string 'not known'\n",
    "    df = df.fillna('Not Known')\n",
    "    \n",
    "    uvdict = {}\n",
    "\n",
    "    for column in df.select_dtypes(exclude=['int','float']):\n",
    "        values_list = df[column].unique()\n",
    "        uvdict[column] = len(values_list)\n",
    "\n",
    "    target_list = list(filter(lambda x: uvdict[x] > threshold, uvdict.keys()))\n",
    "                       \n",
    "                       \n",
    "    for col in target_list:\n",
    "        valued_dict = dict(df[col].value_counts())\n",
    "        safe_values = list(key for key, value in valued_dict.items() if value >= 50)\n",
    "    #     replace_values = list(filter(lambda x: x not in safe_values, all_values))\n",
    "        df.loc[:, col] = df.loc[:, col].map(lambda y: 'other' if y not in safe_values else y)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def bin_me(df):\n",
    "    \"\"\"\n",
    "        creates bins for construction_year based on 5 year increments\n",
    "        inaddition, values stored as year 0 will be transformed to not_available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        basedate = datetime(2020, 1, 1)\n",
    "        a = list(range(1955,2016,5))\n",
    "        cut_bins = [-1]\n",
    "        cut_bins.extend(a)\n",
    "        cut_labels = ['not available', '56-60','61-65','66-70','71-75','76-80','81-85','86-90','91-95','96-00','01-05','06-10','11-15']\n",
    "        df.loc[:, 'construction_year_bin'] = pd.cut(df['construction_year'], bins = cut_bins, labels = cut_labels)\n",
    "        df.drop(['construction_year'], axis = 1, inplace = True)\n",
    "        return df\n",
    "    except:\n",
    "        if 'construction_year_bin' in df.columns:\n",
    "            print('action already performed')\n",
    "        else:\n",
    "            print('you messed up')\n",
    "\n",
    "def onehotmess(df):\n",
    "    df_objects = df.select_dtypes(exclude=['int','float']).drop(['status_group'], axis = 1)\n",
    "    df_nums = df.select_dtypes(include=['int','float'])\n",
    "\n",
    "    df_onehot = pd.get_dummies(df_objects)\n",
    "\n",
    "    df_final = pd.concat([df_nums, df_onehot], axis = 1)\n",
    "    \n",
    "    return df_final, df.status_group\n",
    "\n",
    "def normalize_func(df_values, df_target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, test_size = .05, random_state = 42)\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_ = scaler.fit_transform(X_train)\n",
    "    X_test_ = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_, X_test_, y_train, y_test\n",
    "\n",
    "\n",
    "def do_everything(string1, string2, drop_values, thresh = 200):\n",
    "    \"\"\"this funciton is magical and does everything we could ever want and more\"\"\"\n",
    "    loaded_data = load_data(string1, string2)\n",
    "    df_dropped = drop_useless_cols(loaded_data, drop_values)\n",
    "    fixed_date = fix_dates(df_dropped)\n",
    "    cleaner_df = clean_data(fixed_date, thresh)\n",
    "    df_binned = bin_me(cleaner_df)\n",
    "    ohm_df, target_df = onehotmess(df_binned)\n",
    "    X_train, X_test, y_train, y_test = normalize_func(ohm_df, target_df)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['id', 'funder', 'num_private', 'longitude', 'latitude', 'wpt_name', 'subvillage', 'region_code', 'lga',\n",
    "                'ward','recorded_by', 'scheme_name', 'extraction_type_group', 'payment', 'quality_group', \n",
    "                'quantity_group', 'source_type', 'source_class', 'waterpoint_type_group']\n",
    "\n",
    "X_train, X_test, y_train, y_test = do_everything('./data/training_data_values.csv', './data/training_data_labels.csv',\n",
    "                                                drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4508417508417508"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7841750841750842"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1414,   58,  168],\n",
       "       [ 100,   68,   24],\n",
       "       [ 267,   24,  847]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, knn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=5, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'n_neighbors': [5, 10, 50, 100, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'n_neighbors': [5, 10, 50, 100, 500]}\n",
    "\n",
    "GScv = GridSearchCV(knn, parameters, scoring = 'accuracy', cv = 5)\n",
    "GScv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GScv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
