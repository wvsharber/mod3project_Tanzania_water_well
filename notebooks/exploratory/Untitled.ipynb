{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['amount_tsh', 'date_recorded', 'gps_height', 'installer', 'basin',\n",
       "       'region', 'district_code', 'population', 'public_meeting',\n",
       "       'scheme_management', 'permit', 'construction_year', 'extraction_type',\n",
       "       'extraction_type_class', 'management', 'management_group',\n",
       "       'payment_type', 'water_quality', 'quantity', 'source',\n",
       "       'waterpoint_type', 'status_group'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_values = pd.read_csv('Pump_it_Up_Data_Mining_the_Water_Table_-_Training_set_values.csv')\n",
    "df_labels = pd.read_csv('Pump_it_Up_Data_Mining_the_Water_Table_-_Training_set_labels.csv')\n",
    "\n",
    "\n",
    "drop_columns = ['id', 'funder', 'num_private', 'longitude', 'latitude', 'wpt_name', 'subvillage', 'region_code', 'lga',\n",
    "                'ward','recorded_by', 'scheme_name', 'extraction_type_group', 'payment', 'quality_group', \n",
    "                'quantity_group', 'source_type', 'source_class', 'waterpoint_type_group']\n",
    "\n",
    "continuous_columns = ['amount_tsh', 'date_recorded', 'gps_height', 'population', 'construction_year']\n",
    "\n",
    "categorical_columns = ['installer', 'basin', 'region', 'district_code', 'public_meeting',\n",
    "                      'scheme_management', 'permit', 'extraction_type', 'extraction_type_class', 'management', \n",
    "                      'management_group', 'payment_type', 'water_quality', 'quantity', 'source', 'waterpoint_type']\n",
    "\n",
    "df_original = pd.merge(df_values, df_labels, on = 'id', how = 'inner')\n",
    "\n",
    "df_original.drop(drop_columns, axis = 1, inplace = True)\n",
    "\n",
    "df_original.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_cols(df, drop_values = []):\n",
    "    continuous_columns = ['amount_tsh', 'date_recorded', 'gps_height', 'population', 'construction_year']\n",
    "    for cont in continuous_columns:\n",
    "        if cont in drop_values:\n",
    "            print(f'you cannot drop column: {cont}')\n",
    "            return\n",
    "        \n",
    "    try:\n",
    "        df_dropped = df.drop(drop_values, axis = 1)\n",
    "        return df_dropped\n",
    "    except:\n",
    "        return df\n",
    "    \n",
    "def fix_dates(df):\n",
    "    \"\"\" will take the date of 01/01/2020 and subtract it from the 'date_recorded' column.\n",
    "        This information will be stored in column called 'days_since_recording'\n",
    "        This will also drop the 'date_recorded' column\n",
    "    \"\"\"\n",
    "    basedate = datetime(2020, 1, 1)\n",
    "    df['days_since_recording'] = df.loc[:,'date_recorded'].map(lambda x: (basedate - datetime.strptime(x, \"%Y-%m-%d\")).days)\n",
    "    df.drop(['date_recorded'], axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "def clean_data(df, threshold = 100):\n",
    "    # replaces NaN with a string 'not known'\n",
    "    df = df.fillna('Not Known')\n",
    "    \n",
    "    uvdict = {}\n",
    "\n",
    "    for column in df.select_dtypes(exclude=['int','float']):\n",
    "        values_list = df[column].unique()\n",
    "        uvdict[column] = len(values_list)\n",
    "\n",
    "    target_list = list(filter(lambda x: uvdict[x] > threshold, uvdict.keys()))\n",
    "                       \n",
    "                       \n",
    "    for col in target_list:\n",
    "        valued_dict = dict(df[col].value_counts())\n",
    "        safe_values = list(key for key, value in valued_dict.items() if value >= 50)\n",
    "    #     replace_values = list(filter(lambda x: x not in safe_values, all_values))\n",
    "        df.loc[:, col] = df.loc[:, col].map(lambda y: 'other' if y not in safe_values else y)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def bin_me(df):\n",
    "    \"\"\"\n",
    "        creates bins for construction_year based on 5 year increments\n",
    "        inaddition, values stored as year 0 will be transformed to not_available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        basedate = datetime(2020, 1, 1)\n",
    "        a = list(range(1955,2016,5))\n",
    "        cut_bins = [-1]\n",
    "        cut_bins.extend(a)\n",
    "        cut_labels = ['not available', '56-60','61-65','66-70','71-75','76-80','81-85','86-90','91-95','96-00','01-05','06-10','11-15']\n",
    "        df.loc[:, 'construction_year_bin'] = pd.cut(df['construction_year'], bins = cut_bins, labels = cut_labels)\n",
    "        df.drop(['construction_year'], axis = 1, inplace = True)\n",
    "        return df\n",
    "    except:\n",
    "        if 'construction_year_bin' in df.columns:\n",
    "            print('action already performed')\n",
    "        else:\n",
    "            print('you messed up')\n",
    "\n",
    "def onehotmess(df):\n",
    "    df_objects = df.select_dtypes(exclude=['int','float']).drop(['status_group'], axis = 1)\n",
    "    df_nums = df.select_dtypes(include=['int','float'])\n",
    "\n",
    "    df_onehot = pd.get_dummies(df_objects)\n",
    "\n",
    "    df_final = pd.concat([df_nums, df_onehot], axis = 1)\n",
    "    \n",
    "    return df_final, df.status_group\n",
    "\n",
    "def normalize_func(df_values, df_target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, test_size = .05, random_state = 42)\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_ = scaler.fit_transform(X_train)\n",
    "    X_test_ = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_, X_test_, y_train, y_test\n",
    "\n",
    "\n",
    "def do_everything(string1, string2, drop_values, thresh = 200):\n",
    "    \"\"\"this funciton is magical and does everything we could ever want and more\"\"\"\n",
    "    loaded_data = load_data(string1, string2)\n",
    "    df_dropped = drop_useless_cols(loaded_data, drop_values)\n",
    "    fixed_date = fix_dates(df_dropped)\n",
    "    cleaner_df = clean_data(fixed_date, thresh)\n",
    "    df_binned = bin_me(cleaner_df)\n",
    "    ohm_df, target_df = onehotmess(df_binned)\n",
    "    X_train, X_test, y_train, y_test = normalize_func(ohm_df, target_df)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_everything(string1, string2, drop_values, thresh = 200):\n",
    "    \n",
    "    loaded_data = load_data(string1, string2)\n",
    "    df_dropped = drop_useless_cols(loaded_data, drop_values)\n",
    "    fixed_date = fix_dates(df_dropped)\n",
    "    cleaner_df = clean_data(fixed_date, thresh)\n",
    "    df_binned = bin_me(cleaner_df)\n",
    "    ohm_df, target_df = onehotmess(df_binned)\n",
    "    X_train, X_test, y_train, y_test = normalize_func(ohm_df, target_df)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.03146853, 0.375     , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.03146853, 0.375     , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.50244755, 0.0875    , ..., 0.        , 0.        ,\n",
       "         1.        ],\n",
       "        ...,\n",
       "        [0.        , 0.02692308, 0.0625    , ..., 1.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.03146853, 0.075     , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.48251748, 0.0125    , ..., 0.        , 0.        ,\n",
       "         0.        ]]),\n",
       " array([[0.00000000e+00, 3.14685315e-02, 6.25000000e-02, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 3.14685315e-02, 7.50000000e-02, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [2.85714286e-05, 6.17132867e-01, 1.25000000e-02, ...,\n",
       "         0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 3.14685315e-02, 6.25000000e-02, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 4.90909091e-01, 1.25000000e-02, ...,\n",
       "         0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 3.14685315e-02, 5.00000000e-02, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]),\n",
       " 13719    functional needs repair\n",
       " 47713             non functional\n",
       " 57664                 functional\n",
       " 957                   functional\n",
       " 58484             non functional\n",
       "                   ...           \n",
       " 54343                 functional\n",
       " 38158                 functional\n",
       " 860               non functional\n",
       " 15795                 functional\n",
       " 56422             non functional\n",
       " Name: status_group, Length: 56430, dtype: object,\n",
       " 2980              non functional\n",
       " 5246                  functional\n",
       " 22659                 functional\n",
       " 39888             non functional\n",
       " 13361                 functional\n",
       "                   ...           \n",
       " 15075                 functional\n",
       " 48969                 functional\n",
       " 37665                 functional\n",
       " 22035             non functional\n",
       " 4800     functional needs repair\n",
       " Name: status_group, Length: 2970, dtype: object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_1 = 'Pump_it_Up_Data_Mining_the_Water_Table_-_Training_set_values.csv'\n",
    "string_2 = 'Pump_it_Up_Data_Mining_the_Water_Table_-_Training_set_labels.csv'\n",
    "drop_columns = ['id', 'funder', 'num_private', 'longitude', 'latitude', 'wpt_name', 'subvillage', 'region_code', 'lga',\n",
    "                'ward','recorded_by', 'scheme_name', 'extraction_type_group', 'payment', 'quality_group', \n",
    "                'quantity_group', 'source_type', 'source_class', 'waterpoint_type_group']\n",
    "\n",
    "\n",
    "do_everything(string_1, string_2, drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_1 = 'Pump_it_Up_Data_Mining_the_Water_Table_-_Training_set_values.csv'\n",
    "string_2 = 'Pump_it_Up_Data_Mining_the_Water_Table_-_Training_set_labels.csv'\n",
    "\n",
    "def load_data(string1, string2):\n",
    "    \"\"\"\n",
    "        pass in two strings containg csv info, this function will load the two dataframes and merge them along the column 'id'\n",
    "    \"\"\"\n",
    "    df_1 = pd.read_csv(string1)\n",
    "    df_2 = pd.read_csv(string2)\n",
    "    #merging dataframes\n",
    "    df = pd.merge(df_1, df_2, on = 'id', how = 'inner')\n",
    "    return df\n",
    "\n",
    "loaded_data = load_data(string_1, string_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dates(df):\n",
    "    \"\"\" will take the date of 01/01/2020 and subtract it from the 'date_recorded' column.\n",
    "        This information will be stored in column called 'days_since_recording'\n",
    "        This will also drop the 'date_recorded' column\n",
    "    \"\"\"\n",
    "    basedate = datetime(2020, 1, 1)\n",
    "    df['days_since_recording'] = df.loc[:,'date_recorded'].map(lambda x: (basedate - datetime.strptime(x, \"%Y-%m-%d\")).days)\n",
    "    df.drop(['date_recorded'], axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "fixed_date = fix_dates(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, threshold = 100):\n",
    "    # replaces NaN with a string 'not known'\n",
    "    df = df.fillna('Not Known')\n",
    "    \n",
    "    uvdict = {}\n",
    "\n",
    "    for column in df.select_dtypes(exclude=['int','float']):\n",
    "        values_list = df[column].unique()\n",
    "        uvdict[column] = len(values_list)\n",
    "\n",
    "    target_list = list(filter(lambda x: uvdict[x] > threshold, uvdict.keys()))\n",
    "                       \n",
    "                       \n",
    "    for col in target_list:\n",
    "        valued_dict = dict(df[col].value_counts())\n",
    "        safe_values = list(key for key, value in valued_dict.items() if value >= 50)\n",
    "    #     replace_values = list(filter(lambda x: x not in safe_values, all_values))\n",
    "        df.loc[:, col] = df.loc[:, col].map(lambda y: 'other' if y not in safe_values else y)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "cleaner_df = clean_data(fixed_date)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DWE           17402\n",
       "other          9303\n",
       "Not Known      3655\n",
       "Government     1825\n",
       "RWE            1206\n",
       "              ...  \n",
       "RC Ch            52\n",
       "wanan            52\n",
       "Local te         52\n",
       "RWE/DWE          52\n",
       "CDTF             50\n",
       "Name: installer, Length: 146, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner_df.installer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_me(df):\n",
    "    \"\"\"\n",
    "        creates bins for construction_year based on 5 year increments\n",
    "        inaddition, values stored as year 0 will be transformed to not_available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        basedate = datetime(2020, 1, 1)\n",
    "        a = list(range(1955,2016,5))\n",
    "        cut_bins = [-1]\n",
    "        cut_bins.extend(a)\n",
    "        cut_labels = ['not available', '56-60','61-65','66-70','71-75','76-80','81-85','86-90','91-95','96-00','01-05','06-10','11-15']\n",
    "        df.loc[:, 'construction_year_bin'] = pd.cut(df['construction_year'], bins = cut_bins, labels = cut_labels)\n",
    "        df.drop(['construction_year'], axis = 1, inplace = True)\n",
    "        return df\n",
    "    except:\n",
    "        if 'construction_year_bin' in df.columns:\n",
    "            print('action already performed')\n",
    "        else:\n",
    "            print('you messed up')\n",
    "            \n",
    "df_binned = bin_me(cleaner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotmess(df):\n",
    "    df_objects = df.select_dtypes(exclude=['int','float']).drop(['status_group'], axis = 1)\n",
    "    df_nums = df.select_dtypes(include=['int','float'])\n",
    "\n",
    "    df_onehot = pd.get_dummies(df_objects)\n",
    "\n",
    "    df_final = pd.concat([df_nums, df_onehot], axis = 1)\n",
    "    \n",
    "    return df_final, df.status_group\n",
    "\n",
    "ohm_df, target_df = onehotmess(df_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_func(df_values, df_target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_values, df_target, test_size = .05, random_state = 42)\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_ = scaler.fit_transform(X_train)\n",
    "    X_test_ = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_, X_test_, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = normalize_func(ohm_df, target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
